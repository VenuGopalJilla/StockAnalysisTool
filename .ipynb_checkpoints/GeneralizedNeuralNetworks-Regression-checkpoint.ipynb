{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ruled-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install neupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rapid-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worldwide-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "written-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neupy import algorithms\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-living",
   "metadata": {},
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrative-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(data,null_threshold):\n",
    "    \"\"\"\n",
    "    Drops Date and Unix Date columns from the data.\n",
    "    Drops the columns which has null values more than specified null_threshold.\n",
    "    Replaces infinite values with NAN.\n",
    "    Drops the rows which has null values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "\n",
    "    null_threshold : numeric\n",
    "        numeric value describing the amount of null values that can be present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dataframe\n",
    "        an updated dataframe after performing all the opertaions.\n",
    "    \"\"\"\n",
    "    \n",
    "    data.drop(columns=['Unix Date','Date'],axis=1,inplace=True)\n",
    "    total = data.shape[0]\n",
    "    for col in data.columns:\n",
    "        if null_threshold * total / 100 < data[col].isnull().sum():\n",
    "            data.drop(columns=[col],axis=1,inplace=True)\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    data = data.apply(pd.to_numeric,errors='coerce')\n",
    "    data.dropna(axis=0,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-avatar",
   "metadata": {},
   "source": [
    "# Removing columns based on the dependent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elect-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_column(data,column):\n",
    "    \"\"\"\n",
    "    Removes all the Next Day columns.\n",
    "    Removes all the non Growth Rate Columns (GR)\n",
    "    add the predictor column to list of columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "\n",
    "    column : string\n",
    "        name of the predictor column \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dataframe\n",
    "        an updated dataframe after performing all the opertaions.\n",
    "    column : string\n",
    "        name of the predictor column\n",
    "    \"\"\"\n",
    "    cols = [col for col in data.columns if \"next\" not in col.lower() and col.lower().endswith(\"gr\")]\n",
    "    cols.append(column)\n",
    "    data = data[cols]\n",
    "    return (data,column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-dubai",
   "metadata": {},
   "source": [
    "# Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separate-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(data, target, significance_level=0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]]).astype(float)).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<significance_level):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "buried-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(data, target,significance_level = 0.05):\n",
    "    features = data.columns.tolist()\n",
    "    while(len(features)>0):\n",
    "        features_with_constant = sm.add_constant(data[features]).astype(float)\n",
    "        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        if(max_p_value >= significance_level):\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-bangkok",
   "metadata": {},
   "source": [
    "# GNN and its best parameters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complicated-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestParameters(X,Y,std = 0.1):\n",
    "    model = RandomizedSearchCV(algorithms.GRNN(std=std, verbose=False), param_distributions={'std': np.arange(1e-2, 1, 1e-3)}, scoring='neg_mean_squared_error',)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(preprocessing.minmax_scale(X), preprocessing.minmax_scale(Y), test_size=0.3, random_state=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    return model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spatial-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generalized_neural_network(X,Y,std=0.1):\n",
    "#     best = bestParameters(X,Y,std)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(preprocessing.minmax_scale(X), preprocessing.minmax_scale(Y), test_size=0.3, random_state=0)\n",
    "#     model = algorithms.GRNN(std=best['std'])\n",
    "#     model.train(x_train,y_train)\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     rmse = sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "#     mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "#     mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "#     r2 = metrics.r2_score(y_test, y_pred)\n",
    "#     c = 0\n",
    "#     for a,b in zip(y_test, y_pred):\n",
    "#         if a*b >= 0:\n",
    "#             c += 1\n",
    "#     direction = c/len(y_test)\n",
    "#     myres =  {\"RMSE\":rmse,\"MAE\":mae,\"MSE\":mse,\"rsquared_adj\":r2,\"std\":std, \"best-std\":best['std'],\"Percentage\":direction}\n",
    "#     print(\"done\")\n",
    "#     return myres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "raising-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_neural_network(X,Y,std=0.1):\n",
    "    best = bestParameters(X,Y,std)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "    model = algorithms.GRNN(std=best['std'])\n",
    "    model.train(x_train,y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "#     print(list(zip(y_test, y_pred)))\n",
    "    \n",
    "    y_pred = np.nan_to_num(y_pred)\n",
    "    \n",
    "    rmse = sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    c = 0\n",
    "    for a,b in zip(y_test, y_pred):\n",
    "        if a*b >= 0:\n",
    "            c += 1\n",
    "    direction = c/len(y_test)\n",
    "    myres =  {\"RMSE\":rmse,\"MAE\":mae,\"MSE\":mse,\"rsquared_adj\":r2,\"std\":std, \"best-std\":best['std'],\"Percentage\":direction}\n",
    "#     print(\"done\")\n",
    "    return myres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-photograph",
   "metadata": {},
   "source": [
    "# Finding results from each set of important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "harmful-penny",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' (<ipython-input-12-1489cedca5a3>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-1489cedca5a3>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    results[\"GNN_Regression_FI_\" + method + \"_stds_\" + str(round(s, 2)] = model_result[\"Percentage\"]\u001b[0m\n\u001b[1;37m                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "def fit_GNN(df, column, method, name, results):\n",
    "    print(\"GNN Model fitted using columns obtained from feature importance using \" + method + \" : \")\n",
    "    X = df[df.columns[:-1]]\n",
    "    Y = df[column].values\n",
    "    \n",
    "    data = list()\n",
    "    stds = np.linspace(0,1,11)\n",
    "    arguments = list()\n",
    "    max_percentage = 0\n",
    "    for s in stds:\n",
    "        data = [X,Y,s]\n",
    "        arguments.append(data)\n",
    "        model_result = generalized_neural_network(X, Y, s)\n",
    "        if (max_percentage < model_result[\"Percentage\"]):\n",
    "            max_percentage = model_result[\"Percentage\"]\n",
    "#     threads = ThreadPool(4)\n",
    "#     model_result = threads.starmap(generalized_neural_network,arguments)\n",
    "    \n",
    "#         print(model_result)\n",
    "    \n",
    "#         print(\"Percentage of correct direction : \", model_result[\"Percentage\"])\n",
    "        \n",
    "    \n",
    "        results[\"GNN_Regression_FI_\" + method + \"_stds_\" + str(round(s, 2))] = model_result[\"Percentage\"]\n",
    "    \n",
    "        create_pretty_table(name , \"GNN_Regression\", method + \"-\" + str(round(s, 2)), model_result)\n",
    "    print(\"Maximum percentage of correct direction : \", max_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_FI_ForwardSelection(df1, name, column, results):\n",
    "    print(\"Features Importance using Forward Selection Method\")\n",
    "    print(\"*****************************************************************************************\")\n",
    "    method = \"ForwardSelection\"\n",
    "    X = df1[df1.columns[:-1]]\n",
    "    Y = df1[column].values\n",
    "    forward_features = forward_selection(X,Y)\n",
    "    print(\"Features obtained from Forward Selection method : \") \n",
    "    print(\"--------------------------------------\")\n",
    "    print(forward_features)\n",
    "    if (len(forward_features) != 0):\n",
    "        forward_features.append(column)\n",
    "        df_fs = df1[forward_features]\n",
    "        fit_GNN(df_fs, column, method, name, results)\n",
    "    print(\"*****************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_FI_BackwardElimination(df1, name, column, results):\n",
    "    print(\"Features Importance using Backward Elimination Method\")\n",
    "    print(\"*****************************************************************************************\")\n",
    "    method = \"BackwardElimination\"\n",
    "    X = df1[df1.columns[:-1]]\n",
    "    Y = df1[column].values\n",
    "    backward_features = backward_elimination(X,Y)\n",
    "    print(\"Features obtained from Backward Elimination method : \") \n",
    "    print(\"--------------------------------------\")\n",
    "    print(backward_features)\n",
    "    if (len(backward_features) != 0):\n",
    "        backward_features.append(column)\n",
    "        df_be = df1[backward_features]\n",
    "        fit_GNN(df_be, column, method, name, results)\n",
    "    print(\"*****************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_FI_AllFeatures(df1, name, column, results):\n",
    "    print(\"All Features are considered : \")\n",
    "    print(\"*****************************************************************************************\")\n",
    "    method = \"AllFeaturesConsideration\"\n",
    "    X = df1[df1.columns[:-1]]\n",
    "    Y = df1[column].values\n",
    "    all_features = list(X.columns)\n",
    "    print(\"All Features are --->>\") \n",
    "    print(\"--------------------------------------\")\n",
    "    print(all_features)\n",
    "    if (len(all_features) != 0):\n",
    "        all_features.append(column)\n",
    "        df_all = df1[all_features]\n",
    "        fit_GNN(df_all, column, method, name, results)\n",
    "    print(\"*****************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_each_set(data, name, final_df):\n",
    "    df = pre_process_data(data, 60)\n",
    "    column = \"Next Day Close Price GR\"\n",
    "    (df1, column) = dependent_column(df, column)\n",
    "    results = {}\n",
    "    get_results_from_FI_ForwardSelection(df1, name, column, results)\n",
    "    get_results_from_FI_BackwardElimination(df1, name, column, results)\n",
    "    get_results_from_FI_AllFeatures(df1, name, column, results)\n",
    "    sorted_results = sorted(results.items(), key=lambda item: item[1])\n",
    "    max_row = {'Company' : name[2 : 8] + \"-\" + companies[name[2 : 8]], 'Model' : 'GNN-Regression', 'Method' : sorted_results[-1][0], 'Percentage' : sorted_results[-1][1]}\n",
    "    final_df = final_df.append(max_row, ignore_index = True)\n",
    "    print(\"Maximum correct direction values are obtained for {} with a percentage of {}.\".format(sorted_results[-1][0], sorted_results[-1][1]))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-scout",
   "metadata": {},
   "source": [
    "# Process of getting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretty_table(name, model, method, result):\n",
    "    values = [name[2 : 8 ] + \"-\" + companies[name[2 : 8]], method] + [round(v, 6) if (not isinstance(v, str)) else v for k,v in result.items()]\n",
    "    tables[model].add_row(values)\n",
    "    tables[model].title = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns =['Company','Method', 'RMSE','MAE','MSE','rsquared_adj','std','best-std','Percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = {\"500112\" : \"SBIN\" ,\n",
    "\"500325\" : \"RELIANCE INDUSTRIES LTD\",\n",
    "\"532540\" : \"TATA CONSULTANCY SERVICES LTD\" ,\n",
    "\"500209\" : \"INFOSYS LTD\", \n",
    "\"532174\" : \"ICICI BANK LTD\", \n",
    "\"507685\" : \"WIPRO LTD\", \n",
    "\"530965\" : \"INDIAN OIL CORPORATION LTD\", \n",
    "\"500182\" : \"HERO MOTOCORP LTD\", \n",
    "\"532210\" : \"CITY UNION BANK LTD\", \n",
    "\"500180\" : \"HDFC Bank Ltd\",\n",
    "\"500680\" : \"PFIZER LTD\", \n",
    "\"506395\" : \"COROMANDEL iNTERNATIONAL LTD\",\n",
    "\"500770\" : \"TATA CHEMICALS LTD\", \n",
    "\"500085\" : \"CHAMBAL FERTILISERS & CHEMICALS LTD\", \n",
    "\"501425\" : \"BOMBAY BURMAH TRADING CORP.LTD\", \n",
    "\"532899\" : \"KAVERI SEED COMPANY LTD\", \n",
    "\"537291\" : \"NATH BIO-GENES (INDIA) LTD\", \n",
    "\"500790\" : \"NESTLE INDIA LTD\", \n",
    "\"500825\" : \"BRITANNIA INDUSTRIES LTD\", \n",
    "\"533155\" : \"JUBILANT FOODWORKS LTD\", \n",
    "\"533287\" : \"ZEE LEARN LTD\", \n",
    "\"533260\" : \"CAREER POINT LTD\", \n",
    "\"539921\" : \"SHANTI EDUCATIONAL INITIATIVES LTD\", \n",
    "\"542602\" : \"EMBASSY OFFICE PARKS REIT\", \n",
    "\"543217\" : \"MINDSPACE BUSINESS PARKS REIT\", \n",
    "\"543261\" : \"BROOKFIELD INDIA REAL ESTATE TRUST REIT\", \n",
    "\"532538\" : \"ULTRATECH CEMENT LTD\", \n",
    "\"500387\" : \"SHREE CEMENT LTD\", \n",
    "\"500425\" : \"AMBUJA CEMENTS LTD\", \n",
    "\"532689\" : \"PVR LTD\", \n",
    "\"532706\" : \"INOX LEISURE LTD\", \n",
    "\"532163\" : \"SAREGAMA INDIA LTD\", \n",
    "\"524715\" : \"SUN PHARMACEUTICAL INDUSTRIES LTD\", \n",
    "\"532488\" : \"DIVI'S LABORATORIES LTD\",\n",
    "\"500124\" : \"DR.REDDY'S LABORATORIES LTD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"GNN_Regression\"]\n",
    "tables = {model:PrettyTable() for model in models}\n",
    "for name,table in tables.items():\n",
    "    table.field_names = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"C:\\\\Users\\\\venu\\\\Desktop\\\\Stock Market Analysis\\\\Final_Results_df.csv\")\n",
    "# final_df.drop('Unnamed: 0', inplace = True, axis = 'columns')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for filename in os.listdir(os.path.join(path,\"Data/Stock\")):\n",
    "    if filename.startswith(\"gr\"):\n",
    "        df_gnn = pd.read_csv(os.path.join(path,\"Data\\Stock\\\\\" + filename))\n",
    "        name = os.path.join(path, \"Data\\Stock\\\\\" + filename).split(\"\\\\\")[-1]\n",
    "        stock = name[2 : 8]\n",
    "        fd_df = pd.DataFrame(columns = final_df.columns)\n",
    "        print(\"For stock : \", stock)\n",
    "        print(\"#################################################################################################################\")\n",
    "        f_df = get_results_from_each_set(df_gnn, name, fd_df)\n",
    "        final_df = final_df.append(f_df, ignore_index = True)\n",
    "        print(\"#################################################################################################################\")\n",
    "#         break\n",
    "        \n",
    "final_df = final_df.sort_values(by = ['Company', 'Percentage'], ascending = [True, False])\n",
    "final_df.to_csv('C:\\\\Users\\\\venu\\\\Desktop\\\\Stock Market Analysis\\\\Final_Results_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         filepath = os.path.join(dirname, filename)\n",
    "#         df = pd.read_csv(filepath)\n",
    "#         df = pre_process_data(df,60)\n",
    "#         column = \"Next Day Close Price GR\"\n",
    "#         (df,column) = dependent_column(df,column)\n",
    "#         X = df.drop(columns=[column])\n",
    "#         Y = df[column]\n",
    "        \n",
    "#         resultdf = pd.DataFrame(result)\n",
    "#         resultdf.to_csv(os.path.join(os.getcwd(),str(filename[2:8])+\"_gnn\"+\".csv\"),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,table in tables.items():\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

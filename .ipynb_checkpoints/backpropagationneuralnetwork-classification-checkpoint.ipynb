{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, make_scorer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import sqrt\n",
    "import traceback\n",
    "from multiprocessing.pool import ThreadPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(data,null_threshold):\n",
    "    \"\"\"\n",
    "    Drops Date and Unix Date columns from the data.\n",
    "    Drops the columns which has null values more than specified null_threshold.\n",
    "    Replaces infinite values with NAN.\n",
    "    Drops the rows which has null values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "\n",
    "    null_threshold : numeric\n",
    "        numeric value describing the amount of null values that can be present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dataframe\n",
    "        an updated dataframe after performing all the opertaions.\n",
    "    \"\"\"\n",
    "    \n",
    "    data.drop(columns=['Unix Date','Date'],axis=1,inplace=True)\n",
    "    total = data.shape[0]\n",
    "    for col in data.columns:\n",
    "        if null_threshold * total / 100 < data[col].isnull().sum():\n",
    "            data.drop(columns=[col],axis=1,inplace=True)\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    data = data.apply(pd.to_numeric,errors='coerce')\n",
    "    data.dropna(axis=0,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependent_column(data,column):\n",
    "    \"\"\"\n",
    "    Removes all the Next Day columns.\n",
    "    Removes all the non Growth Rate Columns (GR)\n",
    "    add the predictor column to list of columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe\n",
    "\n",
    "    column : string\n",
    "        name of the predictor column \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dataframe\n",
    "        an updated dataframe after performing all the opertaions.\n",
    "    column : string\n",
    "        name of the predictor column\n",
    "    \"\"\"\n",
    "    cols = [col for col in data.columns if \"next\" not in col.lower() and col.lower().endswith(\"gr\")]\n",
    "    cols.append(column)\n",
    "    data = data[cols]\n",
    "    return (data,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metrics(y_true, y_pred):\n",
    "    rmse = sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    return {\"root_mean_squared_error\":rmse,\"mean_absolute_error\":mae,\"mean_squared_error\":mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(y_pred,y_true):\n",
    "    \n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_true,y_pred)\n",
    "    precision = metrics.precision_score(y_true,y_pred)\n",
    "    recall = metrics.recall_score(y_true,y_pred)\n",
    "    f1_score = metrics.f1_score(y_true,y_pred)\n",
    "    return {\"accuracy\":accuracy,\"precision\":precision,\"recall\":recall,\"f1_score\":f1_score,\"confusion matrix\":cm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpnn_classification(df,column,epochs,batch_size,threshold):\n",
    "    df[\"Target\"] = df[column].apply(lambda x : 1 if x >= threshold else 0)\n",
    "    X = df.drop(columns=[\"Target\",column])\n",
    "    Y = df[\"Target\"]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state = 0)\n",
    "    input_dim = x_train.shape[1]\n",
    "\n",
    "    # return (x_train, x_test, y_train, y_test)\n",
    "    x_train = np.array(x_train)\n",
    "    x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1]))\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1]))\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = np.reshape(y_test,(y_test.shape[0],1))\n",
    "    \n",
    "#     model = KerasClassifier(build_fn = create_model_b, batch_size=batch_size, epochs=epochs,input_dim = input_dim) \n",
    "    model = create_model_b(input_dim)\n",
    "    history = model.fit(x_train,y_train,epochs = epochs,batch_size = batch_size, validation_data = (x_test,y_test),shuffle = False,verbose=0)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.reshape(y_pred,(y_pred.shape[0],1)).round()\n",
    "    \n",
    "    result = {}\n",
    "    error = error_metrics(y_test, y_pred)\n",
    "    confusion = create_confusion_matrix(y_test,y_pred)\n",
    "    result.update(error)\n",
    "    result.update(confusion)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_b(input_dim,layers=3,units = 32):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units,input_dim=(input_dim),activation = 'relu'))\n",
    "    model.add(Dense(units=units,activation = 'relu'))\n",
    "    model.add(Dense(units = 1,activation = 'sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[tf.keras.metrics.Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_params_bpnn(X,Y):\n",
    "    custom_scorer = make_scorer(precision_score, greater_is_better=True,pos_label = 1)\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    X = np.reshape(X,(X.shape[0],X.shape[1]))\n",
    "    Y = np.array(Y)\n",
    "    Y = np.reshape(Y,(Y.shape[0],1))\n",
    "    \n",
    "#     model = create_model_b(input_dim)\n",
    "    model = KerasClassifier(build_fn = create_model_b, batch_size=32, epochs=5,input_dim = input_dim) \n",
    "    \n",
    "    batch_size = [25,32,48,64,100]\n",
    "    epochs = [25,50,75,100]\n",
    "    param_grid = dict(epochs=epochs,batch_size=batch_size)   \n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid,n_jobs=-1,scoring=custom_scorer,verbose=0)\n",
    "    grid_result = grid.fit(X,Y)\n",
    "    batch_size, epochs = grid_result.best_params_['batch_size'],grid_result.best_params_['epochs']\n",
    "    \n",
    "    return batch_size, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpnn_classification(df, column = \"Next Day Close Price GR\"):\n",
    "    threshold = [0.01,0.02,0.03,0.04,0.05,0.1]\n",
    "    solution = list()\n",
    "    for t in threshold:\n",
    "        df[\"Target\"] = df[column].apply(lambda x : 1 if x >= 0.01 else 0)\n",
    "        X = df.drop(columns=[\"Target\",column])\n",
    "        Y = df[\"Target\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "        batch_size, epochs = best_params_bpnn(X_train,y_train)\n",
    "        result = create_bpnn_classification(df,column = \"Next Day Close Price GR\",epochs = epochs,batch_size = batch_size,threshold = t)\n",
    "        result.update({\"batch_size\":batch_size,\"epochs\":epochs,\"threshold\":t})\n",
    "        solution.append(result)\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(\"/kaggle/input/stockdata/all.csv\")\n",
    "inputpath  = \"/kaggle/input/stockdata/Data/Stock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for _,row in total.iterrows():\n",
    "    security_id = row['security id']\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(inputpath,\"gr\"+str(security_id)+\".csv\"))\n",
    "        df = pre_process_data(df,60)\n",
    "        df,column = dependent_column(df, column = \"Next Day Close Price GR\")\n",
    "        bpnn_res = bpnn_classification(df)\n",
    "        bpnn_df = pd.DataFrame(bpnn_res)\n",
    "        bpnn_df.to_csv('bpnn_'+str(security_id)+\".csv\",index=None)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
